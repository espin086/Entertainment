#Replicating the old model and slides in the deck
library(ggplot2) #to create gorgeous graphics
library(caret) #Used for Machine Learning training and validating
setwd("/mnt/apic/BI Team/HQ Release Regression 2016/2. Data/2. Clean Data")
df <- read.csv("regressionraw_2016_03_w_HV v2.csv")
#Regression Slide with confidence intervals
lm.original <- lm(log(WW210) ~ WW.Box + Box...Days.to.HQ + A + Animation + Adventure + Comedy + Drama + Romantic.Comedy + Horror.Thriller + PG.13 + R + Has.CAM., data = df)
summary(lm.original)
confint(lm.original, level = .80)
url <- "http://amshq.org/School-Resources/Find-a-School"
library(XML)
table <-readHTMLTable(url)
url <- "http://www.montessori-namta.org/School-Directory"
table <-readHTMLTable(url)
url <- "http://www.montessoricensus.org/school-maps/public/CA"
table <-readHTMLTable(url)
table[1]
str(table)
url <- "http://www.montessoricensus.org/schools/blue-oak-charter-montessori"
table <-readHTMLTable(url)
setwd("~/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump")
setwd("~/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump")
list.files()
text.v <- scan("RNC Speech.txt", what = "character", sep = "\n")
text.v
list.files()
setwd("~/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump")
text.v <- scan("RNC Speech.txt", what = "character", sep = "\n")
length(text.v)
text.lower.v <- tolower(text.v)
text.words.l <-strsplit(text.lower.v, "\\W")
text.words.l
text.words.v <- unlist(text.words.l)
text.words.v
not.blanks.v <- which(text.words.v !="")
text.words.v <- text.words.v[not.blanks.v]
text.words.v
legth(text.words.v[which(text.words.v == "america")])
length(text.words.v[which(text.words.v == "america")])
america.hits <- length(text.words.v[which(text.words.v == "america")])
total.words <- length(text.words.v)
america.hits / total.words
hits <- length(text.words.v[which(text.words.v == "muslim")])
total.words <- length(text.words.v)
hits / total.words
table(text.words.v)
text.freqs.t <- table(text.words.v)
sorted.text.freqs.t <- sort(text.freqs.t, decreasing = TRUE)
sorted.text.freqs.t
plot(sorted.text.freqs.t[1:10], type = "b",
xlab = "Top Ten Words", ylab = "Total Word Count",
xant = "n")
axis(1:1:10, lables = names(sorted.text.freqs.t[1:10]))
axis(1:10, lables = names(sorted.text.freqs.t[1:10]))
axis(1, 1:10, lables = names(sorted.text.freqs.t[1:10]))
axis(1, 1:10, labels = names(sorted.text.freqs.t[1:10]))
#Plot of top 10 words
plot(sorted.text.freqs.t[1:10], type = "b",
xlab = "Top Ten Words", ylab = "Total Word Count",
xant = "n")
axis(1, 1:10, labels = names(sorted.text.freqs.t[1:10]))
plot(sorted.text.freqs.t[1:10], type = "b",
xlab = "Top Ten Words", ylab = "Total Word Count",
xaxt = "n")
axis(1, 1:10, labels = names(sorted.text.freqs.t[1:10]))
plot(sorted.text.freqs.t[1:10], type = "b", col = "red",
xlab = "Top Ten Words", ylab = "Total Word Count",
xaxt = "n")
axis(1, 1:10, labels = names(sorted.text.freqs.t[1:10]))
sorted.text.rel.freqs.t <- 100*(sorted.text.rel.freqs.t/sum(sorted.text.rel.freqs.t))
sorted.text.rel.freqs.t <- 100*(sorted.text.freqs.t/sum(sorted.text.freqs.t))
plot(sorted.text.rel.freqs.t[1:10], type = "b", col = "red",
xlab = "Top Ten Words", ylab = "Total Word Count",
xaxt = "n")
axis(1, 1:10, labels = names(sorted.text.rel.freqs.t[1:10]))
n.time.v <- seq(1:length(text.words.v))
word.v <- which(text.words.v=="america")
w.count.v <- rep(NA, length(n.time.v))
w.count.v[word.v] <- 1
n.time.v <- seq(1:length(text.words.v)) #creating lenght of text
word.v <- which(text.words.v=="america") #counting word
w.count.v <- rep(NA, length(n.time.v)) #empty vector of NA
w.count.v[word.v] <- 1 #replacing empties with 1 where word is found in text
w.count.v
plot(w.count.v, main = "Dispersion of Plot of 'america' in Trump RNC Speech",
xlab = "Speech Time", ylab = 'america', type = 'h', ylim = c(0,1), yaxt = 'n')
n.time.v <- seq(1:length(text.words.v)) #creating lenght of text
word.v <- which(text.words.v=="mexico") #counting word
w.count.v <- rep(NA, length(n.time.v)) #empty vector of NA
w.count.v[word.v] <- 1 #replacing empties with 1 where word is found in text
plot(w.count.v, main = "Dispersion of Plot of 'america' in Trump RNC Speech",
xlab = "Speech Time", ylab = 'america', type = 'h', ylim = c(0,1), yaxt = 'n')
#Text Distribution Analysis
n.time.v <- seq(1:length(text.words.v)) #creating lenght of text
word.v <- which(text.words.v=="great") #counting word
w.count.v <- rep(NA, length(n.time.v)) #empty vector of NA
w.count.v[word.v] <- 1 #replacing empties with 1 where word is found in text
plot(w.count.v, main = "Dispersion of Plot of 'america' in Trump RNC Speech",
xlab = "Speech Time", ylab = 'america', type = 'h', ylim = c(0,1), yaxt = 'n')
n.time.v <- seq(1:length(text.words.v)) #creating lenght of text
word.v <- which(text.words.v=="the") #counting word
w.count.v <- rep(NA, length(n.time.v)) #empty vector of NA
w.count.v[word.v] <- 1 #replacing empties with 1 where word is found in text
plot(w.count.v, main = "Dispersion of Plot of 'america' in Trump RNC Speech",
xlab = "Speech Time", ylab = 'america', type = 'h', ylim = c(0,1), yaxt = 'n')
n.time.v <- seq(1:length(text.words.v)) #creating lenght of text
word.v <- which(text.words.v=="america") #counting word
w.count.v <- rep(NA, length(n.time.v)) #empty vector of NA
w.count.v[word.v] <- 1 #replacing empties with 1 where word is found in text
plot(w.count.v, main = "Dispersion of Plot of 'america' in Trump RNC Speech",
xlab = "Speech Time", ylab = 'america', type = 'h', ylim = c(0,1), yaxt = 'n')
sum(text.words.v)
sorted.text.rel.freqs.t
sorted.text.freqs.t
sum(sorted.text.freqs.t)
sum(sorted.text.freqs.t)/length(sorted.text.rel.freqs.t)
mean(sorted.text.freqs.t)
sorted.text.freqs.t[2]
str(sorted.text.freqs.t)
hapax <- sapply(sorted.text.freqs.t, function(x) sum(x ==1))
hapax <- sapply(sorted.text.freqs.t, function(x) sum(x ==1))
hapax
hapax / length(text.words.v)
sum(hapax) / length(text.words.v)
sum(sorted.text.freqs.t)
hapax.percentage <- hapax/sum(sorted.text.freqs.t)
hapax.percentage
length(hapax)
hapax.percentage <- length(hapax)/sum(sorted.text.freqs.t)
hapax.percentage
hapax
1247/4416
hapax.percentage
hapax
sum(hapax)
hapax.percentage <- sum(hapax)/sum(sorted.text.freqs.t)
hapax.percentage
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
#1. Not Contacted, 2. Phone Screened, 3. In Person Interview
#0. Test 1, 0. Test 2
candidates <- c("2. Trump")
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
setwd("~/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump")
getwd
getwd()
s.dir <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
s.cor <- VCorpus(DirSource(directory = s.dir), readerControl = list(reader=readPlain))
list.files()
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor.cl <- cleanCorpus(s.cor)
s.cor.cl
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
#1. Not Contacted, 2. Phone Screened, 3. In Person Interview
#0. Test 1, 0. Test 2
s.dir <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
s.cor <- VCorpus(DirSource(directory = s.dir), readerControl = list(reader=readPlain))
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor.cl <- cleanCorpus(s.cor)
str(s.cor.cl)
lapply(s.cor.cl, length)
library(caret) #used to train models
library(doMC)
registerDoMC(cores = 4)
set.seed(2015)
#reading in training and test set
setwd("/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data")
df <- read.csv("TDM.csv")
View(df)
clusters <- hclust(dist(df[, 2:length(df)]))
plot(clusters)
plot(clusters, labels = df$target)
clusterCut <- cutree(clusters, 2)
table(clusterCut, df$target)
clusterCut <- cutree(clusters, 3)
table(clusterCut, df$target)
clusterCut <- cutree(clusters, 4)
table(clusterCut, df$target)
clusterCut <- cutree(clusters, 5)
table(clusterCut, df$target)
clusterCut <- cutree(clusters, 6)
table(clusterCut, df$target)
clusterCut <- cutree(clusters, 10)
table(clusterCut, df$target)
clusterCut <- cutree(clusters, 20)
table(clusterCut, df$target)
clusters <- hclust(dist(df[, 2:length(df)]), method = 'average')
plot(clusters)
plot(clusters,, labels = df$target)
library(caret) #used to train models
library(doMC)
registerDoMC(cores = 4)
set.seed(2015)
#reading in training and test set
setwd("/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data")
df <- read.csv("TDM.csv")
#trying different distance linkage method
clusters <- hclust(dist(df[, 2:length(df)]), method = 'average')
plot(clusters,, labels = df$target)
#cutting off cluster and plotting versus actual
clusterCut <- cutree(clusters, 20)
table(clusterCut, df$target)
clusterCut <- cutree(clusters, 2)
table(clusterCut, df$target)
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor <- VCorpus(DirSource(directory = pathname), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor <- VCorpus(DirSource(directory = pathname), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
str(s.cor.cl)
s.cor.cl
lapply(s.cor.cl, function(x) length(x[which(x == "america")]))
lapply(s.cor.cl, table]))
sapply(s.cor.cl, `[`, "content"))
sapply(s.cor.cl, [, "content"))
sapply(s.cor.cl, "[", "content"))
sapply(s.cor.cl, "[", "content")
list.text <- sapply(s.cor.cl, "[", "content")
list.text <- unlist(sapply(s.cor.cl, "[", "content"))
list.text
gsub("http\\w+", "", s.cor.cl)
list.text <- unlist(sapply(s.cor.cl, "[", "content"))
convert.tm.to.character(s.cor.cl)
s.cor.cl
list.text <- unlist(sapply(s.cor.cl, "[", "content"))
list.text
gsub("http\\w+", "", list.text)
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor <- VCorpus(DirSource(directory = pathname), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
#extracting content from corpus into a list
list.text <- unlist(sapply(s.cor.cl, "[", "content"))
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor <- VCorpus(DirSource(directory = pathname), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
#extracting content from corpus into a list
list.text <- sapply(s.cor.cl, "[", "content")
list.text
list.text.clean <- sapply(list.text, strsplit , "\\W")
list.text.clean
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor <- VCorpus(DirSource(directory = pathname), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
#extracting content from corpus into a list
list.text <- sapply(s.cor.cl, "[", "content")
list.text[[1]]
for (i in 1: lengh(list.text)){
print(i)
}
for (i in 1: length(list.text)){
print(i)
}
for (i in 1: length(list.text)){
strsplit(list.text[[1]], "\\W")
}
list.text[[1]]
list.text.clean <- list()
list.text.clean <- list()
for (i in 1: length(list.text)){
list.text.clean[[i]]<- strsplit(list.text[[1]], "\\W")
}
str(list.text.clean)
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data/2. Trump"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
s.cor <- VCorpus(DirSource(directory = pathname), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
library(caret) #used to train models
library(doMC)
registerDoMC(cores = 4)
set.seed(2015)
#reading in training and test set
setwd("/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data")
df <- read.csv("TDM.csv")
freq <- colSums(as.matrix(df))
freq <- colSums(as.matrix(df[2:length(df)]))
length(freq)
freq
View(df)
summary(df)
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
#1. Not Contacted, 2. Phone Screened, 3. In Person Interview
#0. Test 1, 0. Test 2
candidates <- c("1. Hilary", "2. Trump")
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
#Build a Term-Document-Matrix(TDM)
generateTDM <- function(cand, path){
s.dir <- sprintf("%s/%s", path, cand)
#s.cor <- Corpus(DirSource(directory = s.dir, encoding = "ANSI"))
s.cor <- VCorpus(DirSource(directory = s.dir), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.40)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
inspect(dtms)
inspect(tdm)
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
inspect(tdm)
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
#1. Not Contacted, 2. Phone Screened, 3. In Person Interview
#0. Test 1, 0. Test 2
candidates <- c("2. Trump")
pathname <- "/Users/jjespinoza/Documents/Text Classifiction - 2016 Presidential Position on Issues/1. Data"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
#Build a Term-Document-Matrix(TDM)
generateTDM <- function(cand, path){
s.dir <- sprintf("%s/%s", path, cand)
#s.cor <- Corpus(DirSource(directory = s.dir, encoding = "ANSI"))
s.cor <- VCorpus(DirSource(directory = s.dir), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.40)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
#Exploring Term Document matrix
inspect(tdm)
library(tm)
setwd("~/Documents/HollywoodModels/0. Data/1. Raw Data /0. TV Scripts")
files <- list.files(pattern = "pdf$")
library(tm)
#Directory with TV Scripts
setwd("~/Documents/HollywoodModels/0. Data/1. Raw Data /0. TV Scripts")
files <- list.files(pattern = "pdf$")
Rpdf <- readPDF(control = list(text = "-layout"))
opinions <- Corpus(URISource(files), readerControl = list(reader = Rpdf))
library(tm)
opinions <- Corpus(URISource(files), readerControl = list(reader = Rpdf))
scripts <- Corpus(URISource(files), readerControl = list(reader = Rpdf))
