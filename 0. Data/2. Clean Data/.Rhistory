WIR1 + WIR2 + WIR3 + WIR4 + WIR5,
data=temp,
model = "pooling",
index=c("title", "Date"))
#Fixed Effects Model
fe <- plm(target ~ on1stHQrip +
WIR1 + WIR2 + WIR3 + WIR4 + WIR5,
data=temp,
model = "within",
index=c("title", "Date"))
#Time Fixed Effects Model
tfe <- plm(target ~ on1stHQrip + as.factor(week) +
WIR1 + WIR2 + WIR3 + WIR4 + WIR5 ,
data=temp,
model = "within",
index=c("title", "Date"))
#Random Effects Model
re <- plm(target ~ on1stHQrip + as.factor(week) +
WIR1 + WIR2 + WIR3 + WIR4 + WIR5,
data=temp,
model = "random",
index=c("title", "Date"))
######################################################################
#MODEL DIAGNOSTICS
#Hausman test: if less than .05 then use fixed effects regression
hausman <- phtest(fe, re)
#Testing for Random Effects: (null is no panel effect) if less than .05 then use panel data models (reject null)
bp.test <- plmtest(ols, type=c("bp"))
#Testing for Cross Sectional Independence: if less than .05 then  cross-section dependence exists (bad thing!)
contemporaneous.cor.test <- pcdtest(fe, test = c("cd"))
#Testing for Serial Correlation: if less than .05 then serial correlation exists (bad thing!)
serial.correlation.test <- pbgtest(fe)
#Testing for Unit Roots: if less than .05 then no Unit Root (good thing!)
#panel.set <- plm.data(temp, index = c("title", "Date"))
#unit.root.test <- adf.test(panel.set$target, k=2)
#Testing for homoskedasticity
homo.test <-  bptest(target ~ on1stHQrip + as.factor(title) +
WIR1 + WIR2 + WIR3 + WIR4 + WIR5 +
WIR6, data = temp, studentize=F)
#Using robust standard errors to correct for heteroskedasticity
robust <- coeftest(fe, vcov=vcovHC(fe,type="HC0",cluster="group"))
######################################################################
#WEEKLY IMPACT MODELS
fe.weekly <- plm(target ~
WIR1 + WIR2 + WIR3 + WIR4 + WIR5 +
I(WIR1*on1stHQrip) + I(WIR2*on1stHQrip) + I(WIR3*on1stHQrip) + I(WIR4*on1stHQrip) +
I(WIR5*on1stHQrip),
data=temp,
model = "within",
index=c("title", "Date"))
######################################################################
#CALCULATING LOSSES WITH FINAL MODEL Average decay model
#Loss with OLS
temp$projected.loss.ols <- temp$weekbox * temp$on1stHQrip * ols[[1]][1]
temp$weekbox.no.1st.HQ.ols <- temp$weekbox + (temp$projected.loss.ols * -1)
revenue.loss.ols <- (sum(temp$weekbox.no.1st.HQ.ols) - sum(as.numeric(temp$weekbox)))/sum(as.numeric(temp$weekbox))
#Loss with OLS
temp$projected.loss.fe <- temp$weekbox * temp$on1stHQrip * fe[[1]][1]
temp$weekbox.no.1st.HQ.fe <- temp$weekbox + (temp$projected.loss.fe * -1)
revenue.loss.fe <- (sum(temp$weekbox.no.1st.HQ.fe) - sum(as.numeric(temp$weekbox)))/sum(as.numeric(temp$weekbox))
#Loss with TFE
temp$projected.loss.tfe <- temp$weekbox * temp$on1stHQrip * tfe[[1]][1]
temp$weekbox.no.1st.HQ.tfe <- temp$weekbox + (temp$projected.loss.tfe * -1)
revenue.loss.tfe <- (sum(temp$weekbox.no.1st.HQ.tfe) - sum(as.numeric(temp$weekbox)))/sum(as.numeric(temp$weekbox))
#Loss with RE
temp$projected.loss.RE <- temp$weekbox * temp$on1stHQrip * re[[1]][1]
temp$weekbox.no.1st.HQ.re <- temp$weekbox + (temp$projected.loss.RE * -1)
revenue.loss.re <- (sum(temp$weekbox.no.1st.HQ.re) - sum(as.numeric(temp$weekbox)))/sum(as.numeric(temp$weekbox))
######################################################################
#PRINTING THE RESULTS
print('#########################################################')
print("COUNTRY")
print(paste0("Country of Analysis: ", x))
print('#########################################################')
print("MODEL RESULTS")
print('RESULTS USING THE OLS MODEL')
print(paste0("The decay on BO after Box Office is: ", ols[[1]][1]))
print(paste0("Box Office would have been higher by: ", revenue.loss.ols, " if there would have been no HQ leak"))
print('RESULTS USING THE FIXED EFFECTS MODEL')
print(paste0("The decay on BO after Box Office is: ", fe[[1]][1]))
print(paste0("Box Office would have been higher by: ", revenue.loss.fe, " if there would have been no HQ leak"))
print('RESULTS USING THE TIME FIXED EFFECTS MODEL')
print(paste0("The decay on BO after Box Office is: ", tfe[[1]][1]))
print(paste0("Box Office would have been higher by: ", revenue.loss.tfe, " if there would have been no HQ leak"))
print('RESULTS USING THE RANDOM EFFECTS MODEL')
print(paste0("The decay on BO after Box Office is: ", re[[1]][1]))
print(paste0("Box Office would have been higher by: ", revenue.loss.re, " if there would have been no HQ leak"))
print('#########################################################')
print('REGRESSION DIAGNOSTICS')
print("Used Fixed Effects if Hausman belows is less than 0.05, otherwise use RE")
print(hausman)
print("Used Panel Data models if BP test is less than 0.05, otherwise use OLS")
print(bp.test)
print("Correct for conteporaneous correlation if BP LM test is less than 0.05, otherwise all good")
print(contemporaneous.cor.test)
print("Correct for serial correlation if this test is less than 0.05, otherwise all good")
print(serial.correlation.test)
#print("No Unit Root if this test is less than 0.05, otherwise should correct")
#print(unit.root.test)
print("Correct for heteroskedasticity if this test is less than 0.05, otherwise all good")
print(homo.test)
print('#########################################################')
print('REGRESSION MODEL WITH ROBUST STANDARD ERRORS')
print('Corrects for: Heteroskedasticity and arbitrary forms of serial correlation')
print(robust)
print('#########################################################')
print('FINAL MODEL AVERAGE RESULTS')
print(summary(fe))
print('#########################################################')
print('FINAL MODEL WEEKLY RESULTS')
print(summary(fe.weekly))
}
Bretts.China("China")
library(car)#visualization
library(plm)#panel data
library(gplots)#Additional visualizations
library(plyr)#data manupulation
library(tseries)
library(lmtest)
setwd("~/Documents/Early-Release-Regression-International/2. Data")
CleanPanel <- read.csv("CleanInternationalPanel.csv")
CleanPanel <- CleanPanel[which(CleanPanel$year == 2015 | CleanPanel$year == 2014 | CleanPanel$year == 2013 | CleanPanel$year == 2012),]
#CleanPanel <- CleanPanel[CleanPanel$dist != "Buena Vista",]
myvars <- c("title", "Date", "week", "territory", "weekbox","onCAMrip", "on1stHQrip", "on2ndHQrip","onHVrip", "ratingR", "ratingPG", "week.since.1stHQ" , "WIR", "WIR0", "WIR1", "WIR2", "WIR3", "WIR4", "WIR5", "WIR6", "WIR7" , "WIR8", "WIR9")
CleanPanel <- CleanPanel[myvars]
#Brazil
#Indonesia
#Italy
#Japan
#Mexico
#Netherlands
#Russia
#Singapore
#Spain
#Thailand
#second.rip.week <- CleanPanel[which(CleanPanel$week.since.2ndHQ == 1),]
#hist(second.rip.week$WIR)
#summary(second.rip.week$WIR)
Bretts <- function(x){
#Filtering dataset by Country
temp <- CleanPanel[which(CleanPanel$territory == x),]
#Removing title/date dups (only 1 in Australia)
dups <- duplicated(temp[1:2])
temp <- temp[!dups,]
#Logarithmic Transformation of dependent variable
target <- log(temp$weekbox + 1)
######################################################################
#CREATING THE DIFFERENT MODELS
#Fixed Effects Model
fe <- plm(target ~ on1stHQrip +
WIR1 + WIR2 + WIR3 + WIR4 + WIR5 +
WIR6 + WIR7 + WIR8 + WIR9 ,
data=temp,
model = "within",
index=c("title", "Date"))
######################################################################
#CALCULATING LOSSES WITH FINAL MODEL Average decay model
#Loss with FE
temp$projected.loss.fe <- temp$weekbox * temp$on1stHQrip * fe[[1]][1]
temp$weekbox.no.1st.HQ.fe <- temp$weekbox + (temp$projected.loss.fe * -1)
revenue.loss.fe <- (sum(temp$weekbox.no.1st.HQ.fe) - sum(as.numeric(temp$weekbox)))/sum(as.numeric(temp$weekbox))
######################################################################
#PRINTING THE RESULTS
print('#########################################################')
print("COUNTRY")
print(paste0("Country of Analysis: ", x))
print('#########################################################')
print("MODEL RESULTS")
print('RESULTS USING THE FIXED EFFECTS MODEL')
print(paste0("The decay on BO after Box Office is: ", fe[[1]][1]))
print(paste0("Box Office would have been higher by: ", revenue.loss.fe, " if there would have been no HQ leak"))
print('#########################################################')
print('FINAL MODEL AVERAGE RESULTS')
print(summary(fe))
}
Bretts("Domestic US/Canada")
Bretts("United Kingdom")
Bretts("France")
Bretts("Germany")
Bretts("Japan")
Bretts("Korea")
Bretts("Australia")
library(car)#visualization
library(plm)#panel data
library(gplots)#Additional visualizations
library(plyr)#data manupulation
library(tseries)
library(lmtest)
setwd("~/Documents/Early-Release-Regression-International/2. Data")
CleanPanel <- read.csv("CleanInternationalPanel.csv")
CleanPanel <- CleanPanel[which(CleanPanel$year == 2015 | CleanPanel$year == 2014 ,]
Bretts("Domestic US/Canada")
Bretts("United Kingdom")
Bretts("France")
Bretts("Germany")
Bretts("Japan")
Bretts("Domestic US/Canada")
Bretts("United Kingdom")
Bretts("France")
Bretts("Germany")
#Bretts("Japan")
Bretts("Korea")
Bretts("Australia")
Bretts("Domestic US/Canada")
Bretts("United Kingdom")
Bretts("France")
Bretts("Germany")
#Bretts("Japan")
Bretts("Korea")
Bretts("Australia")
library(car)#visualization
library(plm)#panel data
library(gplots)#Additional visualizations
library(plyr)#data manupulation
library(tseries)
library(lmtest)
setwd("~/Documents/Early-Release-Regression-International/2. Data")
CleanPanel <- read.csv("CleanInternationalPanel.csv")
CleanPanel <- CleanPanel[which(CleanPanel$year == 2015 | CleanPanel$year == 2014 ,]
CleanPanel <- CleanPanel[which(CleanPanel$year == 2015 | CleanPanel$year == 2014 ,]
library(car)#visualization
library(plm)#panel data
library(gplots)#Additional visualizations
library(plyr)#data manupulation
library(tseries)
library(lmtest)
setwd("~/Documents/Early-Release-Regression-International/2. Data")
CleanPanel <- read.csv("CleanInternationalPanel.csv")
CleanPanel <- CleanPanel[which(CleanPanel$year == 2015 | CleanPanel$year == 2014) ,]
#CleanPanel <- CleanPanel[CleanPanel$dist != "Buena Vista",]
myvars <- c("title", "Date", "week", "territory", "weekbox","onCAMrip", "on1stHQrip", "on2ndHQrip","onHVrip", "ratingR", "ratingPG", "week.since.1stHQ" , "WIR", "WIR0", "WIR1", "WIR2", "WIR3", "WIR4", "WIR5", "WIR6", "WIR7" , "WIR8", "WIR9")
CleanPanel <- CleanPanel[myvars]
#Brazil
#Indonesia
#Italy
#Japan
#Mexico
#Netherlands
#Russia
#Singapore
#Spain
#Thailand
#second.rip.week <- CleanPanel[which(CleanPanel$week.since.2ndHQ == 1),]
#hist(second.rip.week$WIR)
#summary(second.rip.week$WIR)
Bretts <- function(x){
#Filtering dataset by Country
temp <- CleanPanel[which(CleanPanel$territory == x),]
#Removing title/date dups (only 1 in Australia)
dups <- duplicated(temp[1:2])
temp <- temp[!dups,]
#Logarithmic Transformation of dependent variable
target <- log(temp$weekbox + 1)
######################################################################
#CREATING THE DIFFERENT MODELS
#Fixed Effects Model
fe <- plm(target ~ on1stHQrip +
WIR1 + WIR2 + WIR3 + WIR4 + WIR5 +
WIR6 + WIR7 + WIR8 + WIR9 ,
data=temp,
model = "within",
index=c("title", "Date"))
######################################################################
#CALCULATING LOSSES WITH FINAL MODEL Average decay model
#Loss with FE
temp$projected.loss.fe <- temp$weekbox * temp$on1stHQrip * fe[[1]][1]
temp$weekbox.no.1st.HQ.fe <- temp$weekbox + (temp$projected.loss.fe * -1)
revenue.loss.fe <- (sum(temp$weekbox.no.1st.HQ.fe) - sum(as.numeric(temp$weekbox)))/sum(as.numeric(temp$weekbox))
######################################################################
#PRINTING THE RESULTS
print('#########################################################')
print("COUNTRY")
print(paste0("Country of Analysis: ", x))
print('#########################################################')
print("MODEL RESULTS")
print('RESULTS USING THE FIXED EFFECTS MODEL')
print(paste0("The decay on BO after Box Office is: ", fe[[1]][1]))
print(paste0("Box Office would have been higher by: ", revenue.loss.fe, " if there would have been no HQ leak"))
print('#########################################################')
print('FINAL MODEL AVERAGE RESULTS')
print(summary(fe))
}
Bretts("Domestic US/Canada")
Bretts("United Kingdom")
Bretts("France")
Bretts("Germany")
Bretts("Korea")
Bretts("Australia")
library(car)#visualization
library(plm)#panel data
library(gplots)#Additional visualizations
library(plyr)#data manupulation
library(tseries)
library(lmtest)
setwd("~/Documents/Early-Release-Regression-International/2. Data")
CleanPanel <- read.csv("CleanInternationalPanel.csv")
CleanPanel <- CleanPanel[which(CleanPanel$year == 2015 | CleanPanel$year == 2014 | CleanPanel$year == 2013) ,]
#CleanPanel <- CleanPanel[CleanPanel$dist != "Buena Vista",]
myvars <- c("title", "Date", "week", "territory", "weekbox","onCAMrip", "on1stHQrip", "on2ndHQrip","onHVrip", "ratingR", "ratingPG", "week.since.1stHQ" , "WIR", "WIR0", "WIR1", "WIR2", "WIR3", "WIR4", "WIR5", "WIR6", "WIR7" , "WIR8", "WIR9")
CleanPanel <- CleanPanel[myvars]
#Brazil
#Indonesia
#Italy
#Japan
#Mexico
#Netherlands
#Russia
#Singapore
#Spain
#Thailand
#second.rip.week <- CleanPanel[which(CleanPanel$week.since.2ndHQ == 1),]
#hist(second.rip.week$WIR)
#summary(second.rip.week$WIR)
Bretts <- function(x){
#Filtering dataset by Country
temp <- CleanPanel[which(CleanPanel$territory == x),]
#Removing title/date dups (only 1 in Australia)
dups <- duplicated(temp[1:2])
temp <- temp[!dups,]
#Logarithmic Transformation of dependent variable
target <- log(temp$weekbox + 1)
######################################################################
#CREATING THE DIFFERENT MODELS
#Fixed Effects Model
fe <- plm(target ~ on1stHQrip +
WIR1 + WIR2 + WIR3 + WIR4 + WIR5 +
WIR6 + WIR7 + WIR8 + WIR9 ,
data=temp,
model = "within",
index=c("title", "Date"))
######################################################################
#CALCULATING LOSSES WITH FINAL MODEL Average decay model
#Loss with FE
temp$projected.loss.fe <- temp$weekbox * temp$on1stHQrip * fe[[1]][1]
temp$weekbox.no.1st.HQ.fe <- temp$weekbox + (temp$projected.loss.fe * -1)
revenue.loss.fe <- (sum(temp$weekbox.no.1st.HQ.fe) - sum(as.numeric(temp$weekbox)))/sum(as.numeric(temp$weekbox))
######################################################################
#PRINTING THE RESULTS
print('#########################################################')
print("COUNTRY")
print(paste0("Country of Analysis: ", x))
print('#########################################################')
print("MODEL RESULTS")
print('RESULTS USING THE FIXED EFFECTS MODEL')
print(paste0("The decay on BO after Box Office is: ", fe[[1]][1]))
print(paste0("Box Office would have been higher by: ", revenue.loss.fe, " if there would have been no HQ leak"))
print('#########################################################')
print('FINAL MODEL AVERAGE RESULTS')
print(summary(fe))
}
Bretts("Domestic US/Canada")
Bretts("United Kingdom")
Bretts("France")
Bretts("Germany")
Bretts("Korea")
Bretts("Australia")
summary(df)
View(df)
View(df)
library(sqldf)
sqldf("SELECT * from df")
str(df)
sqldf("SELECT * from df where No.1.Movie > 30")
sqldf("SELECT * from df where No_1_
Movie > 30")
cars
sqldf("SELECT * from cars where speed > 10")
df <- sqldf("SELECT * from cars where speed > 10")
View(df)
df <- sqldf("SELECT * from cars where speed > 10 AND dist > 50")
View(df)
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
options(stringsAsFactors = FALSE)
candidates <- c("0. Blockbuster (75M+)","0. Not Blockbuster (<75M)")
pathname <- "/Users/jjespinoza/Documents/HollywoodModels/0. Data/1. Raw Data /0. TV Scripts - 2011-2015/"
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
#1. Not Contacted, 2. Phone Screened, 3. In Person Interview
#0. Test 1, 0. Test 2
candidates <- c("0. Blockbuster (75M+)","0. Not Blockbuster (<75M)")
pathname <- "/Users/jjespinoza/Documents/HollywoodModels/0. Data/1. Raw Data /0. TV Scripts - 2011-2015/"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
#Build a Term-Document-Matrix(TDM)
generateTDM <- function(cand, path){
s.dir <- sprintf("%s/%s", path, cand)
#s.cor <- Corpus(DirSource(directory = s.dir, encoding = "ANSI"))
s.cor <- VCorpus(DirSource(directory = s.dir), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.05)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
?VCorpus
??VCorpus
?train
??train
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
#1. Not Contacted, 2. Phone Screened, 3. In Person Interview
#0. Test 1, 0. Test 2
candidates <- c("0. Blockbuster (75M+)","0. Not Blockbuster (<75M)")
pathname <- "/Users/jjespinoza/Documents/HollywoodModels/0. Data/1. Raw Data /0. TV Scripts - 2011-2015/"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
#Build a Term-Document-Matrix(TDM)
generateTDM <- function(cand, path){
s.dir <- sprintf("%s/%s", path, cand)
#s.cor <- Corpus(DirSource(directory = s.dir, encoding = "ANSI"))
s.cor <- VCorpus(DirSource(directory = s.dir), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.05)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
#attach interview result to matrix
bindCandidateToTDM <- function(tdm){
s.mat <- t(data.matrix(tdm[["tdm"]]))
s.df <- as.data.frame(s.mat, stringsAsFactors = FALSE)
s.df <- cbind(rep(tdm[["name"]], nrow(s.df)), s.df)
#colnames(s.df)[ncol(s.df)] <- "interviewresult"
}
candTDM <- lapply(tdm, bindCandidateToTDM)
#stack
tdm.stack <- do.call(rbind.fill, candTDM)
tdm.stack[is.na(tdm.stack)] <- 0
#Renaming target variable
colnames(tdm.stack)[1] <- "target"
#Exporting Clean Dataset
setwd("~/Documents/HollywoodModels/0. Data/2. Clean Data")
write.csv(tdm.stack, "TDM_Scripts.csv", row.names = FALSE)
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
#set options
options(stringsAsFactors = FALSE)
#set parameters
#1. Not Contacted, 2. Phone Screened, 3. In Person Interview
#0. Test 1, 0. Test 2
candidates <- c("0. Blockbuster (75M+)","0. Not Blockbuster (<75M)")
pathname <- "/Users/jjespinoza/Documents/HollywoodModels/0. Data/1. Raw Data /0. TV Scripts - 2011-2015/"
#Clean text
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
return(corpus.tmp)
}
#Build a Term-Document-Matrix(TDM)
generateTDM <- function(cand, path){
s.dir <- sprintf("%s/%s", path, cand)
#s.cor <- Corpus(DirSource(directory = s.dir, encoding = "ANSI"))
s.cor <- VCorpus(DirSource(directory = s.dir), readerControl = list(reader=readPlain))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.05)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
#attach interview result to matrix
bindCandidateToTDM <- function(tdm){
s.mat <- t(data.matrix(tdm[["tdm"]]))
s.df <- as.data.frame(s.mat, stringsAsFactors = FALSE)
s.df <- cbind(rep(tdm[["name"]], nrow(s.df)), s.df)
#colnames(s.df)[ncol(s.df)] <- "interviewresult"
}
candTDM <- lapply(tdm, bindCandidateToTDM)
#stack
tdm.stack <- do.call(rbind.fill, candTDM)
tdm.stack[is.na(tdm.stack)] <- 0
#Renaming target variable
colnames(tdm.stack)[1] <- "target"
#Exporting Clean Dataset
setwd("~/Documents/HollywoodModels/0. Data/2. Clean Data")
write.csv(tdm.stack, "TDM_Scripts.csv", row.names = FALSE)
install.packages("tm")
#WARNING: Convert all data into text and ensure ANSI encoding can use this site to convert:  http://utils.paranoiaworks.org/diacriticsremover/
#PDF to text site
#http://pdftotext.com/
#init
libs<- c("tm", "plyr", "class", "reshape")
lapply(libs, require, character.only = TRUE)
